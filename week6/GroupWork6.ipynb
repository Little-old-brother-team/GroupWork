{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can define the second difference matrix as follows ($N=5$ for example):\n",
    "   $$\n",
    "    A = \\begin{pmatrix}\n",
    "    2 &-1  &  & &\\\\\n",
    "     -1 & 2 &-1  & \\\\\n",
    "     & -1 & 2 &-1  \\\\\n",
    "     & & -1  &2 &-1 \\\\\n",
    "     & & & -1 & 2\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    a) Let the true solution $x_a$ be generated by the normal distributed random numbers, thus $b = Ax$  \n",
    "    b) The convergence condition can be set as $||x_{n+1} − x_n|| \\leq \\epsilon$ for some small value of $\\epsilon$. Choose your own $\\epsilon$, try to modify the sample script `jacobi_method.ipynb` to count how many loop iterations occur before convergence. How does it change with the change of $N$?  \n",
    "    c) The weighted Jacobi method uses a parameter $\\omega$ to compute the iteration as\n",
    "   $$\n",
    "    \\mathbf {x} ^{(k+1)}=\\left(1-\\omega \\right)\\mathbf {x} ^{(k)}+ \\omega\\mathbf {x} ^{(k+1)}_{\\text{Jacobi}}=\\left(1-\\omega \\right)\\mathbf {x} ^{(k)}+ \\omega D^{-1}(\\mathbf {b} -(L+U)\\mathbf {x} ^{(k)})\n",
    "   $$\n",
    "    with $\\omega =2/3$ being the usual choice. Modify your script to implement the weighted Jacobi method. Does it converge faster than the Jacobi method?\n",
    "    Hint: You can read *Jacobi_method_Wikipedia.pdf* for your reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "A = \n[[ 2. -1.  0.  0.  0.]\n [-1.  2. -1.  0.  0.]\n [ 0. -1.  2. -1.  0.]\n [ 0.  0. -1.  2. -1.]\n [ 0.  0.  0. -1.  2.]]\nx_a =\n [[-0.12649358]\n [-0.36391951]\n [ 0.69134178]\n [-0.77279592]\n [ 0.35723972]]\nb = \n[[ 0.11093235]\n [-1.29268722]\n [ 2.51939899]\n [-2.59417334]\n [ 1.48727537]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def Generate_A_b(N):     # 定义一个产生我们要求的A,x_a,b的函数\n",
    "    I = np.eye(N)\n",
    "    L = np.zeros((N,N))\n",
    "    for i in range(1,N):\n",
    "        L[i,i-1] = 1\n",
    "    U = L.T\n",
    "    A = 2*I - L - U\n",
    "    x_a = np.zeros((N,1))\n",
    "    for i in range(N):\n",
    "        x_a[i,0] = random.normalvariate(0,1)\n",
    "    b = A@x_a\n",
    "    return A,x_a,b\n",
    "\n",
    "(A,x_a,b) = Generate_A_b(5)\n",
    "print(f'A = \\n{A}')\n",
    "print(f'x_a =\\n {x_a}')\n",
    "print(f'b = \\n{b}')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "for N = 5\nA = \n[[ 2. -1.  0.  0.  0.]\n [-1.  2. -1.  0.  0.]\n [ 0. -1.  2. -1.  0.]\n [ 0.  0. -1.  2. -1.]\n [ 0.  0.  0. -1.  2.]]\nx_a =\n [[ 1.19890435]\n [ 0.5806933 ]\n [-0.48211893]\n [-2.38904984]\n [ 0.38416309]]\nb = \n[[ 1.8171154 ]\n [ 0.44460117]\n [ 0.84411868]\n [-4.68014383]\n [ 3.15737601]]\nx:\nmatrix([[ 1.19890674],\n        [ 0.58070138],\n        [-0.48211415],\n        [-2.38904176],\n        [ 0.38416548]])\niteration times: 86\n------------------------------------------------------------\nfor N = 7\nA = \n[[ 2. -1.  0.  0.  0.  0.  0.]\n [-1.  2. -1.  0.  0.  0.  0.]\n [ 0. -1.  2. -1.  0.  0.  0.]\n [ 0.  0. -1.  2. -1.  0.  0.]\n [ 0.  0.  0. -1.  2. -1.  0.]\n [ 0.  0.  0.  0. -1.  2. -1.]\n [ 0.  0.  0.  0.  0. -1.  2.]]\nx_a =\n [[-2.86854762]\n [ 1.03536327]\n [ 0.86150152]\n [ 0.49835787]\n [-1.01222819]\n [-1.40633634]\n [-0.43973774]]\nb = \n[[-6.77245851]\n [ 4.07777264]\n [ 0.18928189]\n [ 1.14744243]\n [-1.11647792]\n [-1.36070674]\n [ 0.52686085]]\nx:\nmatrix([[-2.86854372],\n        [ 1.03536717],\n        [ 0.86151092],\n        [ 0.49836339],\n        [-1.01221879],\n        [-1.40633244],\n        [-0.43973385]])\niteration times: 154\n------------------------------------------------------------\nfor N = 10\nA = \n[[ 2. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n [-1.  2. -1.  0.  0.  0.  0.  0.  0.  0.]\n [ 0. -1.  2. -1.  0.  0.  0.  0.  0.  0.]\n [ 0.  0. -1.  2. -1.  0.  0.  0.  0.  0.]\n [ 0.  0.  0. -1.  2. -1.  0.  0.  0.  0.]\n [ 0.  0.  0.  0. -1.  2. -1.  0.  0.  0.]\n [ 0.  0.  0.  0.  0. -1.  2. -1.  0.  0.]\n [ 0.  0.  0.  0.  0.  0. -1.  2. -1.  0.]\n [ 0.  0.  0.  0.  0.  0.  0. -1.  2. -1.]\n [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  2.]]\nx_a =\n [[ 0.04669941]\n [ 0.2793184 ]\n [ 0.51373841]\n [ 1.31242358]\n [ 0.75501937]\n [ 0.21791524]\n [ 0.07007392]\n [-0.24641242]\n [-0.19516474]\n [-1.67642295]]\nb = \n[[-1.85919575e-01]\n [-1.80103449e-03]\n [-5.64265149e-01]\n [ 1.35608938e+00]\n [-2.03000842e-02]\n [-3.89262809e-01]\n [ 1.68645028e-01]\n [-3.67734036e-01]\n [ 1.53250590e+00]\n [-3.15768117e+00]]\nx:\nmatrix([[ 0.04671131],\n        [ 0.2793432 ],\n        [ 0.51377033],\n        [ 1.31246531],\n        [ 0.75506116],\n        [ 0.21796065],\n        [ 0.07011233],\n        [-0.24637775],\n        [-0.19514191],\n        [-1.67641003]])\niteration times: 240\n------------------------------------------------------------\nfor N = 15\nA = \n[[ 2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [-1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2.]]\nx_a =\n [[-0.17800879]\n [ 0.77930693]\n [-0.73562375]\n [ 1.30563615]\n [ 0.21072322]\n [ 1.8830331 ]\n [ 1.23263039]\n [ 0.37107541]\n [ 0.79187929]\n [-1.59759675]\n [ 1.9354957 ]\n [-0.59577311]\n [-0.14294894]\n [ 0.80847726]\n [-1.72932854]]\nb = \n[[-1.13532451]\n [ 2.4722464 ]\n [-3.55619057]\n [ 3.13617282]\n [-2.76722282]\n [ 2.3227126 ]\n [ 0.21115227]\n [-1.28235885]\n [ 2.81027991]\n [-5.92256848]\n [ 6.06436126]\n [-2.98409299]\n [-0.49860203]\n [ 3.489232  ]\n [-4.26713434]]\nx:\nmatrix([[-0.17800739],\n        [ 0.77931101],\n        [-0.73561975],\n        [ 1.30564367],\n        [ 0.21072919],\n        [ 1.88304294],\n        [ 1.23263744],\n        [ 0.37108605],\n        [ 0.79188633],\n        [-1.59758691],\n        [ 1.93550167],\n        [-0.59576559],\n        [-0.14294495],\n        [ 0.80848133],\n        [-1.72932714]])\niteration times: 580\n------------------------------------------------------------\n"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from numpy import array, zeros, diag, diagflat, dot\n",
    "\n",
    "def jacobi(A, b, eps = 1e-5, N = 1000 ,x=None):\n",
    "    \"\"\"Solves the equation Ax=b via the Jacobi iterative method.\"\"\"\n",
    "    # Create an initial guess if needed                                                     \n",
    "    if x is None:\n",
    "        x = np.ones((np.shape(A)[0],1))\n",
    "    # Create a vector of the diagonal elements of A\n",
    "    # and subtract them from A\n",
    "\n",
    "    D = np.matrix(diag(A)).T\n",
    "    R = A - diagflat(D)\n",
    "    \n",
    "    xlast = x\n",
    "    for i in range(N):\n",
    "        x = (b - dot(R,x)) / D\n",
    "        if np.linalg.norm(x-xlast) < eps:\n",
    "            break\n",
    "        xlast = x\n",
    "    \n",
    "    return x, i+1 \n",
    "\n",
    "NN = [5,7,10,15]        # test the iteration times when N = 5, 7, 10, 15\n",
    "for N in NN:\n",
    "    print(f'for N = {N}')\n",
    "    (A,x_a,b) = Generate_A_b(N)\n",
    "    if N == 5:\n",
    "        store = (A,x_a,b)\n",
    "    print(f'A = \\n{A}')\n",
    "    print(f'x_a =\\n {x_a}')\n",
    "    print(f'b = \\n{b}')\n",
    "    sol = jacobi(A,b)\n",
    "    print(\"x:\"); pprint(sol[0])\n",
    "    print(f'iteration times: {sol[1]}')\n",
    "    print(60*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, iteration times grows with $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x:\nmatrix([[ 1.19893255],\n        [ 0.58074214],\n        [-0.48206253],\n        [-2.38900099],\n        [ 0.38419129]])\niteration times: 110\n"
    }
   ],
   "source": [
    "def jacobi_weight(A, b, weight, eps = 1e-5, N = 1000 ,x=None):\n",
    "    # Create an initial guess if needed                                                     \n",
    "    if x is None:\n",
    "        x = np.ones((np.shape(A)[0],1))\n",
    "    # Create a vector of the diagonal elements of A\n",
    "    # and subtract them from A\n",
    "\n",
    "    D = np.matrix(diag(A)).T\n",
    "    R = A - diagflat(D)\n",
    "    \n",
    "    xlast = x\n",
    "    for i in range(N):\n",
    "        x = (1 - weight) * x + weight * (b - dot(R,x)) / D\n",
    "        if np.linalg.norm(x-xlast) < eps:\n",
    "            break\n",
    "        xlast = x\n",
    "\n",
    "    return x, i+1 \n",
    "\n",
    "(A,b) = store[0],store[2]\n",
    "sol = jacobi_weight(A,b,2.0/3)\n",
    "\n",
    "print(\"x:\"); pprint(sol[0])\n",
    "print(f'iteration times: {sol[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the same precision $|x_{n+1} - x_{n}|\\lt 1e-5$, jacobi method iteration times is **89**, the weighted Jacobi method interation times is **101**, so the weighted Jacobi method isn't faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Apply the Gauss-Seidel method to solve the linear equations given in Problem 1.\n",
    "   a)  Choose your own $\\epsilon$, try to modify the sample script `gauss_seidel_method.ipynb` to count the executing time before convergence. How does it change with the change of $N$? \n",
    "   b) Does Gauss-Seidel method converge, as expected, faster than the Jacobi method?\n",
    "   Hint: You can read *Gauss_Seidel_method_Wikipedia.pdf* for your reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# gauss seidel method:\n",
    "def gauss_seidel(A, b, eps=1e-5, N=1000, x=None):\n",
    "    dim = np.shape(A)[0]\n",
    "    if x == None:               # if no epsific start point, set to all zero\n",
    "        x = np.ones(dim)\n",
    "    x_last = np.ones(dim)\n",
    "    for i in range(dim):\n",
    "        sum_xi = 0\n",
    "        for j in range(dim):\n",
    "            if j == i:\n",
    "                continue\n",
    "            sum_xi -= A[i, j] * x[j]\n",
    "        x[i] = (b[i] + sum_xi) / A[i, i]\n",
    "    \n",
    "    x_last[:] = x[:]\n",
    "    for n in range(N):\n",
    "        for i in range(dim):\n",
    "            sum_xi = 0\n",
    "            for j in range(dim):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                sum_xi -= A[i, j] * x[j]\n",
    "            x[i] = (b[i] + sum_xi) / A[i, i]\n",
    "        if np.linalg.norm(x - x_last) < eps:\n",
    "            break\n",
    "        x_last[:] = x[:]  \n",
    "\n",
    "\n",
    "    return x, n + 1\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "for N = 5\nA = \n[[ 2. -1.  0.  0.  0.]\n [-1.  2. -1.  0.  0.]\n [ 0. -1.  2. -1.  0.]\n [ 0.  0. -1.  2. -1.]\n [ 0.  0.  0. -1.  2.]]\nx_a =\n [[ 0.12520141]\n [ 1.2477339 ]\n [-0.28440147]\n [-0.43263429]\n [-0.84875971]]\nb = \n[[-0.99733109]\n [ 2.65466787]\n [-1.38390256]\n [ 0.2678926 ]\n [-1.26488513]]\ngauss seidel: \nx:\narray([ 0.12521069,  1.24774782, -0.28438756, -0.43262385, -0.84875449])\niteration times: 40\njacobi: \nx:\nmatrix([[ 0.12520515],\n        [ 1.24773642],\n        [-0.28439399],\n        [-0.43263178],\n        [-0.84875597]])\niteration times: 86\n------------------------------------------------------------\nfor N = 7\nA = \n[[ 2. -1.  0.  0.  0.  0.  0.]\n [-1.  2. -1.  0.  0.  0.  0.]\n [ 0. -1.  2. -1.  0.  0.  0.]\n [ 0.  0. -1.  2. -1.  0.  0.]\n [ 0.  0.  0. -1.  2. -1.  0.]\n [ 0.  0.  0.  0. -1.  2. -1.]\n [ 0.  0.  0.  0.  0. -1.  2.]]\nx_a =\n [[-1.14245361]\n [ 0.51449179]\n [ 0.43349826]\n [ 1.48203493]\n [-1.8228445 ]\n [ 0.65131731]\n [ 0.2727374 ]]\nb = \n[[-2.79939902]\n [ 1.73793894]\n [-1.12953022]\n [ 4.35341611]\n [-5.77904124]\n [ 2.85274172]\n [-0.10584252]]\ngauss seidel: \nx:\narray([-1.1424401 ,  0.51451486,  0.4335261 ,  1.48206278, -1.82282073,\n        0.65133412,  0.2727458 ])\niteration times: 66\njacobi: \nx:\nmatrix([[-1.14245175],\n        [ 0.51449188],\n        [ 0.43350274],\n        [ 1.48203506],\n        [-1.82284001],\n        [ 0.6513174 ],\n        [ 0.27273925]])\niteration times: 164\n------------------------------------------------------------\nfor N = 10\nA = \n[[ 2. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n [-1.  2. -1.  0.  0.  0.  0.  0.  0.  0.]\n [ 0. -1.  2. -1.  0.  0.  0.  0.  0.  0.]\n [ 0.  0. -1.  2. -1.  0.  0.  0.  0.  0.]\n [ 0.  0.  0. -1.  2. -1.  0.  0.  0.  0.]\n [ 0.  0.  0.  0. -1.  2. -1.  0.  0.  0.]\n [ 0.  0.  0.  0.  0. -1.  2. -1.  0.  0.]\n [ 0.  0.  0.  0.  0.  0. -1.  2. -1.  0.]\n [ 0.  0.  0.  0.  0.  0.  0. -1.  2. -1.]\n [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  2.]]\nx_a =\n [[-1.2372146 ]\n [-1.71068856]\n [-1.0017761 ]\n [-0.70106392]\n [ 1.17459414]\n [ 0.0531269 ]\n [-0.20722729]\n [ 1.22477335]\n [-0.49330447]\n [-0.18531254]]\nb = \n[[-0.76374064]\n [-1.18238642]\n [ 0.40820029]\n [-1.57494589]\n [ 2.99712531]\n [-0.86111306]\n [-1.69235483]\n [ 3.15007846]\n [-2.02606975]\n [ 0.12267939]]\ngauss seidel: \nx:\narray([-1.23719797, -1.71065794, -1.00173502, -0.70101649,  1.17464367,\n        0.05317442, -0.20718539,  1.22480675, -0.49328154, -0.18530108])\niteration times: 122\njacobi: \nx:\nmatrix([[-1.23719942],\n        [-1.71065765],\n        [-1.00173539],\n        [-0.70101191],\n        [ 1.17464746],\n        [ 0.05318349],\n        [-0.20717829],\n        [ 1.22481656],\n        [-0.49327535],\n        [-0.18529643]])\niteration times: 246\n------------------------------------------------------------\nfor N = 15\nA = \n[[ 2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [-1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2. -1.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  2.]]\nx_a =\n [[-0.58200205]\n [-0.53635314]\n [ 0.86703442]\n [ 0.45282565]\n [ 1.1473483 ]\n [-0.2576032 ]\n [ 1.56090741]\n [ 1.1125236 ]\n [ 1.36991082]\n [ 2.38289182]\n [-0.62786612]\n [-0.51155589]\n [-1.65444393]\n [-1.33968185]\n [ 0.21132416]]\nb = \n[[-0.62765095]\n [-1.35773866]\n [ 1.81759633]\n [-1.10873143]\n [ 2.09947417]\n [-3.22346212]\n [ 2.26689442]\n [-0.70577103]\n [-0.75559378]\n [ 4.02373895]\n [-3.12706818]\n [ 1.25919827]\n [-1.45765012]\n [-1.23624393]\n [ 1.76233018]]\ngauss seidel: \nx:\narray([-0.58198283, -0.53631617,  0.86708706,  0.45289136,  1.14742409,\n       -0.25752062,  1.5609934 ,  1.11260959,  1.36999354,  2.38296825,\n       -0.62779866, -0.51149962, -1.65440057, -1.33965256,  0.21133881])\niteration times: 233\njacobi: \nx:\nmatrix([[-0.58199725],\n        [-0.53634506],\n        [ 0.86704807],\n        [ 0.45284058],\n        [ 1.14736874],\n        [-0.2575837 ],\n        [ 1.56093151],\n        [ 1.11254471],\n        [ 1.36993492],\n        [ 2.38291133],\n        [-0.62784569],\n        [-0.51154096],\n        [-1.65443028],\n        [-1.33967377],\n        [ 0.21132896]])\niteration times: 527\n------------------------------------------------------------\n"
    }
   ],
   "source": [
    "# test the iteration times when N = 5, 7, 10, 15 for gauss seidle method\n",
    "NN = [5,7,10,15]       \n",
    "for N in NN:\n",
    "    print(f'for N = {N}')\n",
    "    (A,x_a,b) = Generate_A_b(N)\n",
    "    print(f'A = \\n{A}')\n",
    "    print(f'x_a =\\n {x_a}')\n",
    "    print(f'b = \\n{b}')\n",
    "    sol_g = gauss_seidel(A,b)\n",
    "    sol_j = jacobi(A,b)\n",
    "    print(\"gauss seidel: \")\n",
    "    print(\"x:\"); pprint(sol_g[0])\n",
    "    print(f'iteration times: {sol_g[1]}')\n",
    "    print(\"jacobi: \")\n",
    "    print(\"x:\"); pprint(sol_j[0])\n",
    "    print(f'iteration times: {sol_j[1]}')\n",
    "    print(60*'-')\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The method of successive over-relaxation (SOR) is a variant of the Gauss–Seidel method for solving a linear system of equations, resulting in faster convergence, where the iterative is defined as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\mathbf{x}^{(k+1)}&= (1-\\omega)\\mathbf{x}^{(k)}+\\omega \\mathbf{x}^{(k+1)}_{\\text{Gauss-Seidel}}\\\\\n",
    "   x_i^{(k+1)}&=(1-\\omega)x_i^{(k)}+\\frac{\\omega}{a_{ii}}\\left(b_i-\\displaystyle\\sum_{j<i}a_{ij}x_j^{(k+1)}-\\displaystyle\\sum_{j>i}a_{ij}x_j^{(k)}\\right)\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   where $0<\\omega <2$. $\\omega >1$ is called the relaxation factor. \n",
    "   Consider the linear system as follows:\n",
    "   $$\n",
    "   \\begin{cases}\n",
    "   3x_1 - x_2+ x_3 = -1\\\\\n",
    "   -x_1 +3x_2 -x_3 = 7\\\\\n",
    "   x_1 - x_2 +3x_3 = -7\\\\\n",
    "      \\end{cases}\n",
    "   $$\n",
    "   a) Set your own $\\epsilon$, try to modify the sample script `gauss_seidel_method.ipynb` to implement the SOR method.\n",
    "   b) Can you find the optimal value of $\\omega$ empirically? Please compare your result with the theoretical prediction - the optimal relaxation parameter $\\omega$ is given by\n",
    "    $$\n",
    "    \\omega_\\text{opt} = 1+\\left(\\frac{\\lambda_J}{1+\\sqrt{1-\\lambda_J^2}}\\right)^2 \n",
    "    $$\n",
    "    $\\lambda_J$ is the spectral radius calculated  in the Jacobi method.   \n",
    "    Hint: You can read *SOR_method_Wikipedia.pdf* for your reference. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. One application of the conjugate gradient method is to solve the normal equation to find the least-square solution of an (over-determined) equation system $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, where the coefficient matrix $\\mathbf{A}$ is $M$ by $N$ of rank $n$, i.e., $M\\geq N=n$.The normal equation of this system is \n",
    "$$\n",
    "\\mathbf{A}^{T}\\mathbf{A}\\mathbf{x}=\\mathbf{A}^{T}\\mathbf{b}% \n",
    "$$\n",
    "Here $\\mathbf{A}^{T}\\mathbf{A}$ is an $N$ by $N$ symmetric, positive definite matrix. When $A$ is a large-sized sparse symmetrical matrix, conjugate gradient method is also preferred. \n",
    "Modify the sample script  `conj_grad.ipynb` to solve Problem 1. Please compare your results with the results from the Jacobi method and the Cholesky method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Read Example 7.2[^1] and the sample script `power_method_eig.ipynb`. The problem is to find the largest eigenvalue of matrix $A$:\n",
    "   $$\n",
    "   A = \\begin{pmatrix}\n",
    "   4 & -i & 2 \\\\\n",
    "   i & 2 & 2+7i \\\\\n",
    "   2 & 2-7i & -2\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "  a) Try modifying the above example to count how many loop iterations occur before convergence. Is it larger or smaller than you had expected?  \n",
    "  b) An alternative to using the Rayleigh quotient to determine whether convergence is achieved is the convergence criterion \n",
    "   $$\n",
    "   ||x_{n+1} − x_n|| \\leq \\epsilon\n",
    "   $$\n",
    "    This is especially useful if we do not need to compute the eigenvalue, and can save some computational time. Modify the code above to use this convergence criterion, and compare the overall wall time to the original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dominant eigenvector: [0.33980972-0.23445694j 0.49134274+0.51067232j 0.50105933-0.27621538j]\nDominant eigenvalue: 8.45188-0.00000j\niteration times:  157\n"
    }
   ],
   "source": [
    "\n",
    "# function to calculate the Rayleigh quotient\n",
    "def rayleigh_quotient(A,x):\n",
    "    return np.dot(x, np.dot(A, x))/np.dot(x,x)\n",
    "\n",
    "# function to normalise a vector\n",
    "def normalise(x,eps=1e-10):\n",
    "    N = np.sqrt(np.sum(abs(x)**2))\n",
    "    if N < eps: # in case it is the zero vector!\n",
    "        return x\n",
    "    else:\n",
    "        return x/N\n",
    "\n",
    "A = np.array([[4, -1j, 2],\n",
    "        [1j, 2, 2+7j],\n",
    "        [2, 2-7j, -2]])\n",
    "\n",
    "def find_eig(A, eps=1e-5, x=None):\n",
    "    # choose the starting vector\n",
    "    if x == None:       # if not specify a initial point\n",
    "        x = normalise(np.array([1, 1, 1]))\n",
    "    RQnew = rayleigh_quotient(A,x)\n",
    "    RQold = 0\n",
    "\n",
    "    # perform the power iteration\n",
    "    cnt = 0\n",
    "    while np.abs(RQnew-RQold) > eps:\n",
    "        RQold = RQnew\n",
    "        x = normalise(np.dot(A, x))\n",
    "        RQnew = rayleigh_quotient(A, x)\n",
    "        cnt += 1\n",
    "    return RQnew, x, cnt\n",
    "\n",
    "RQnew, x, cnt = find_eig(A)\n",
    "print(\"Dominant eigenvector:\",x)\n",
    "print(\"Dominant eigenvalue: {:.5f}\".format(RQnew))\n",
    "print(\"iteration times: \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dominant eigenvector: [0.33980994-0.2344566j  0.49134139+0.51067459j 0.50105691-0.27621799j]\nDominant eigenvalue: 8.45188-0.00000j\niteration times:  116\n"
    }
   ],
   "source": [
    "def find_eig2(A, eps=1e-5, N=1000, x=None):\n",
    "    # choose the starting vector\n",
    "    if x == None:\n",
    "        x = normalise(np.array([1, 1, 1]))\n",
    "        x_last = np.copy(x)\n",
    "    # perform the power iteration\n",
    "    for cnt in range(N):\n",
    "        x = normalise(np.dot(A, x))\n",
    "        if np.linalg.norm(x - x_last) < eps:\n",
    "            break\n",
    "        x_last = np.copy(x)\n",
    "    return RQnew, x, cnt\n",
    "\n",
    "RQnew, x, cnt = find_eig2(A)\n",
    "print(\"Dominant eigenvector:\",x)\n",
    "print(\"Dominant eigenvalue: {:.5f}\".format(RQnew))\n",
    "print(\"iteration times: \", cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall time of new method is faster than the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We can use the normalised power method above on $A^{−1}$ to calculate $\\lambda_N$ — the smallest magnitude eigenvalue of $A$, and the dominant eigenvalue of $A^{−1}$. In practice, however, matrix inversion is a difficult numerical computation, is prone to error, and can be quite unstable. A more common approach than direct matrix inversion is to view the inverse power iteration as a system of linear equations to be solved involving $A$:  \n",
    "Step 1. Choose a normalised starting vector $x_0$ where $||x_0|| = 1$;   \n",
    "Step 2. Solve the system of linear equations $Ax_{n+1} = x_n$ to determine the next vector in the iteration, and normalise this vector;  \n",
    "Step 3. Repeat step 2 until convergence is achieved, $||x_{n+1} − x_n|| \\leq \\epsilon$ for some small value of $\\epsilon$. The Rayleigh quotient $x_n^\\dagger Ax_n$ will provide the value of the smallest magnitude eigenvector of $A$.    \n",
    "Modify the sample script to find the smallest eigenvalue of matrix $A$.\n",
    "\n",
    "[^1]: Joshua Izaac, Jingbo Wang, Computational Quantum Mechanics, Springer, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Smallest eigenvalue: 3.18960-0.00000j\nEigenvector correspond:\n [ 0.76568347+0.48507841j  0.1333804 -0.35521551j -0.13264612-0.1298629j ]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conjugate_residual(A, b, x=None):  \n",
    "    \"\"\"\n",
    "    conjugate residual method to solve Hermitian matrix\n",
    "    \\(this method come from Wikipedia, url: https://en.wikipedia.org/wiki/Conjugate_residual_method\\)\n",
    "    \"\"\"\n",
    "    # if use Gauss method, it's equal to calculate the inverse of A\n",
    "    n = len(b)\n",
    "    if not x:\n",
    "        x = np.ones(n)\n",
    "\n",
    "    x = x\n",
    "    r = b - np.dot(A,x)\n",
    "    p = r\n",
    "    \n",
    "    for i in range( 2 * n ):\n",
    "        rAr = np.dot(np.conj(r),np.dot(A,r))\n",
    "        Ap = np.dot(A,p)\n",
    "        alpha = rAr / np.dot( np.conj(Ap) , Ap )\n",
    "        \n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "\n",
    "        rAr_plus_one = np.dot( np.conj(r) ,np.dot(A,r))\n",
    "        beta = rAr_plus_one / rAr\n",
    "\n",
    "        \n",
    "        if abs(np.linalg.norm(r)) < 1e-5:\n",
    "            break\n",
    "\n",
    "        p = r + beta * p\n",
    "\n",
    "    return x\n",
    "\n",
    "# function to calculate the Rayleigh quotient\n",
    "def rayleigh_quotient(A,x):\n",
    "    return np.dot(x, np.dot(A, x))/np.dot(x,x)\n",
    "\n",
    "# function to normalise a vector\n",
    "def normalise(x,eps=1e-10):\n",
    "    N = np.sqrt(np.sum(abs(x)**2))\n",
    "    if N < eps: # in case it is the zero vector!\n",
    "        return x\n",
    "    else:\n",
    "        return x/N\n",
    "\n",
    "A = np.array([[4, -1j, 2],\n",
    "        [1j, 2, 2+7j],\n",
    "        [2, 2-7j, -2]])\n",
    "\n",
    "# choose the starting vector\n",
    "x = normalise(np.array([1, 1, 1]))\n",
    "RQnew = rayleigh_quotient(A,x)\n",
    "RQold = 0\n",
    "\n",
    "# perform the power iteration\n",
    "while np.abs(RQnew-RQold) > 1e-6:\n",
    "    RQold = RQnew\n",
    "    x = normalise(conjugate_residual(A, x))\n",
    "    RQnew = rayleigh_quotient(A, x)\n",
    "\n",
    "print(\"Smallest eigenvalue: {:.5f}\".format(RQnew))\n",
    "print(\"Eigenvector correspond:\\n\",x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}